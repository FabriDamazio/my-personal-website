%{
    title: "Neovim 与本地大语言模型",
    author: "Fabricio Damazio",
    tags: ~w(programming),
    description: "我在 Neovim 中使用 Ollama 和 CodeCompanion 插件的体验"
}
---

在当前围绕大语言模型（LLMs）、智能体和氛围编程（vibe coding）的狂热中，人们很容易迷失方向。那种觉得自己落后或错过了下一个革命性工具的感觉一直存在。

说到使用 AI 进行编程，我可以形容自己是一个谨慎的人。我通常会仔细评估使用的方式和时机。与对承诺的生产力效益感到兴奋相比，我更担心它可能对我的学习造成的危害。

为了寻求一种更 conscious 和受控的 AI 使用方式，本周我开始使用一个在本地运行并与 Neovim 集成的大语言模型。我想谈谈这方面的体验。

## 动机

我决定启动这个项目是因为，在本地运行大语言模型能很好地回应我对于使用 AI 的主要担忧。第一个也是最重要的担忧是关于隐私。在本地处理我的数据在意识形态上是一个优先事项（无需为此支付任何费用是一个加分项）。第二点是关于拥有对我自己环境的控制权，可以选择模型以及何时使用它们。最后，是几乎瞬时的响应速度，这让整个体验非常流畅。

我有一台配置丰实的台式电脑：AMD 9950x CPU、RTX 4090 显卡和 32GB DDR5 内存。拥有这样的计算能力，利用运行本地大语言模型的好处是非常合理的。

## 在本地运行模型

[Ollama](https://ollama.com/){: target="_blank" .font-medium .text-pink-800 } 是我选择用来在本地运行模型的工具。其简单程度令人惊讶：在 Linux 上只需一个命令安装，再用另一个命令下载模型并运行它。

从那以后，我一直在测试不同的模型。我想为每种类型的任务找到一两个专业模型。例如，为了进行 Elixir 和 Rust 编程，我一直在测试 `qwen3:30b-a3b-instruct-2507-q4_K_M` 模型。我选择了一个 300 亿参数（30B）的模型，因为它提供了良好的推理能力，并且 `q4_K_M` 量化使其能舒适地放入我 4090 显卡的显存（VRAM）中，而不会牺牲太多质量。结果令人惊讶，我打算在接下来的几个月里继续测试不同的模型。

## 与 Neovim 集成

我选择了 [CodeCompanion](https://github.com/olimorris/codecompanion.nvim){: target="_blank" .font-medium .text-pink-800 } 插件来实现 Ollama 和 Neovim 之间的集成。安装和配置很简单，我基本上遵循了文档说明。

我发现这种体验与我测试 Zed 编辑器时的体验相似。只需一个命令，聊天面板就会打开，你可以与模型互动。聊天允许添加文件作为对话的上下文。在聊天中，可以更改要使用的模型（本地或远程）。

还可以使用工具来执行各种任务，例如搜索文件、编辑文件、执行命令等。

值得访问该插件的网站并观看可用视频，以了解该插件能够做什么。

## 我目前的体验

时间仍然很短，但到目前为止的体验相当积极。正如我所说，我对使用 AI 非常谨慎，因此我的使用仅限于查阅文档、理论概念以及debate我正在思考的解决方案。

我不使用工具来直接为我编辑代码或解决错误。我坚信我不应该将我对解决方案和代码的理解外包给 AI，但我仍然认为它将帮助我拥有更流畅的开发流程。现在下结论还为时过早，但我们拭目以待，看看这是否会随着时间的推移而成立。

我所有的 Neovim 配置代码都可以在[这个代码库](https://github.com/FabriDamazio/nvim){: target="_blank" .font-medium .text-pink-800 }中找到。

---

*注：本文采用AI辅助翻译，经过人工校对。欢迎反馈建议*
